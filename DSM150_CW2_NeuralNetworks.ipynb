{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<center><h1>IMBD Classification Movie Reviews - DL</h1></center>\n",
    "\n",
    "___\n",
    "\n",
    "<center><h2>DSM0150 - Neural Networks</h2></center><br>\n",
    "<center><strong>Teacher:</strong> Tim Blackwell</center>\n",
    "\n",
    "___\n",
    "<p></p>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\"><strong>Presented by:</strong></center>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\">Jorge Forero L.</center>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\">Student Number: 240323983</center>\n",
    "<center style=\"color: #AA6373; font-weight: 400;\">Student Portal Username: JEFL1</center>\n",
    "<center>March 2025</center>\n",
    "<p></p>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Problem Statement\n",
    "<p></p>\n",
    "This project aims to classify movie reviews in the IMDB dataset as positive or negative using a fully connected (Dense) neural network architecture constrained by Dropout layers, following the universal workflow in Deep Learning with Python (Part 1). By systematically analyzing textual data, tuning hyperparameters, and applying targeted regularization, we seek to identify the primary factors affecting sentiment classification accuracy and mitigate overfitting. Specifically, we investigate how different configurations of layers, units, dropout rates, optimizers, and batch sizes influence model performance, and we examine the extent to which early stopping and dropout help stabilize validation accuracy.\n",
    "\n",
    "Through this process, we develop a robust predictive model that addresses three core questions: \n",
    "    - how architectural choices and parameter tuning impact accuracy\n",
    "    - how dropout-based regularization reduces overfitting\n",
    "    - how these findings can guide best practices for sentiment analysis using only Dense and Dropout layers.\n",
    "\n",
    "The complete analysiswill be version-controlled and hosted on GitHub for easy access and collaboration. You can view and contribute to the project at the following URL: https://github.com/jforeroluque/DSM150_NeuralNetworks_CW1.\n",
    "\n",
    "<p></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aims and Objectives\n",
    "<p></p>\n",
    "The primary aim of this project is to develop a high-performing sentiment classification model on the IMDB dataset, leveraging fully connected (Dense) layers and Dropout for regularization. We seek to accurately distinguish between positive and negative reviews while minimizing overfitting and maintaining model interpretability.\n",
    "<p></p>\n",
    "\n",
    "#### Objectives\n",
    "<p></p>\n",
    "\n",
    "1. Experiment with varying numbers of Dense layers, hidden units, dropout rates, and optimizers to strike an optimal balance between performance, generalizability, and training efficiency.\n",
    "2. Identify both obvious and hidden patterns in the data using innovative visualization and clustering techniques to reveal insights into delivery risks.\n",
    "3. Examine how Dropout (and possibly early stopping or other forms of regularization) can mitigate overfitting, ensuring that the model generalizes well to unseen data.\n",
    "4. Compare the tuned model’s accuracy and loss metrics to a common-sense baseline (50% accuracy) and simpler architectures, evaluating the real impact of regularization and hyperparameter tuning.\n",
    "\n",
    "<p></p>\n",
    "\n",
    "#### Ethical Considerations\n",
    "<p></p>\n",
    "\n",
    "Transparency and Reproducibility\n",
    "All steps of data preprocessing, model design, and hyperparameter tuning have been documented to ensure that others can replicate and validate the findings. Code and plots are presented in a notebook format for clarity and reproducibility.\n",
    "\n",
    "For this Coursework we will be improving the results obtained in the previous one, where we got the following results:\n",
    "\n",
    "Top 5 configurations:\n",
    "Layers=3, Units=32, Dropout=0.5, Optimizer=adam, BatchSize=1024 -> val_acc=0.8910\n",
    "Layers=2, Units=32, Dropout=0.2, Optimizer=rmsprop, BatchSize=1024 -> val_acc=0.8907\n",
    "Layers=3, Units=16, Dropout=0.5, Optimizer=adam, BatchSize=512 -> val_acc=0.8904\n",
    "Layers=2, Units=16, Dropout=0.2, Optimizer=adam, BatchSize=1024 -> val_acc=0.8892\n",
    "Layers=2, Units=16, Dropout=0.5, Optimizer=rmsprop, BatchSize=512 -> val_acc=0.8891\n",
    "\n",
    "30/30 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.9733 - loss: 0.0830 - val_accuracy: 0.8851 - val_loss: 0.5093\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Modules\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding [1]\n",
    "\n",
    "In this phase, we collect, describe, and explore the IMDB movie review dataset to gain insights into its structure and primary attributes. The dataset includes 50,000 reviews, split evenly into training and test sets (25,000 each). Each review is presented as a sequence of word indices, representing the words and their frequencies in the movie review text. For our project, we focus on the top 10,000 most frequently used words to reduce sparsity and maintain manageable vector sizes. This IMDB data supports a quantitative evaluation of our hypotheses on how neural network architectures (specifically Dense and Dropout layers) can effectively classify sentiment, revealing which hyperparameters—such as the number of units, dropout rate, or optimizer—are most influential in achieving strong generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations and constrains of the Data\n",
    "\n",
    "While the IMDB dataset is a valuable resource for exploring sentiment classification, it presents several limitations and constraints that may influence our analysis and model performance:\n",
    "\n",
    "1. Although we limit the vocabulary to the top 10,000 words for practical modeling, this approach can exclude less frequent but potentially significant words. As a result, some nuances in language usage may be lost, potentially leading to an oversimplified representation of the full sentiment space.\n",
    "2. Converting each review into a multi-hot vector of word occurrences does not preserve word order or context. This simplification may limit the model’s ability to leverage sequence-based nuances (e.g., negation words or sarcasm), though this trade-off is acceptable under the constraints of using only Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) # Reference [2] page 95 DLWP\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequence Lenght Analysis**\n",
    "\n",
    "Each review is a list of integers (word indices). We can look at how many words each review contains to understand the distribution of sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lengths = [len(sequence) for sequence in x_train]\n",
    "\n",
    "print(f\"Minimum review length: {np.min(review_lengths)}\")\n",
    "print(f\"Maximum review length: {np.max(review_lengths)}\")\n",
    "print(f\"Average review length: {np.mean(review_lengths):.2f}\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(review_lengths, bins=50, color='blue')\n",
    "plt.title(\"Distribution of Review Lengths (Number of Words)\")\n",
    "plt.xlabel(\"Review Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "Since we already did the model using only dense layers now we want to \n",
    "Since we are restricted to Dense layers, we will need to convert integer-encoded reviews to multi-hot vectors (or one-hot vectors) of shape (10000,). For this our approach will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Neural Network Model\n",
    "\n",
    "Here we will be building a model that uses an Embedding layer to learn a dense representation for words, we also will be applying a bidirectional LSTM to capture sequence dependencies from both directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD model architecture\n",
    "model_rnn = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=128, input_length=maxlen),\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.5)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model architecture\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training & validation sets\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "#Model Training with Callbacks\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True) # Reference [2] page 97 DLWP\n",
    "\n",
    "history_rnn = model_rnn.fit(\n",
    "    partial_x_train_rnn,\n",
    "    partial_y_train_rnn,\n",
    "    epochs=10,              \n",
    "    batch_size=1024,\n",
    "    validation_data=(x_val_rnn, y_val_rnn),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model on test data\n",
    "\n",
    "test_loss, test_acc = model_rnn.evaluate(x_test_padded, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Training and Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history_rnn.history\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history_dict['loss'], 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, history_dict['val_loss'], 'ro-', label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history_dict['accuracy'], 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, history_dict['val_accuracy'], 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signs of Overfitting**\n",
    "\n",
    "We can see that the training loss continously decreases, approaching near-zero, whereas the validation loss bottoms out around epochs 4-5, then starts climbing. This is a classic sign that the network is like \"memorizing\" the training data and losing generalizability.\n",
    "\n",
    "Also we can see that the training accuracy keeps rising surpassing 95% and eventually approachinv 100%. However, the validation accuracy peaks around epochs 4-5 (roughly 88% to 90%) and then starts to decline gradually. This gap between training and validation performance widens with further training, again indicating overfitting.\n",
    "\n",
    "**Remedies**\n",
    "\n",
    "One straightforward approach is to monitor validation loss (or validation accuracy) and stop training as soon as it ceases to improve, in this case around epoch 4 or 5. This prevents the model from overfitting further.\n",
    "Another approach is to use regularization techniques, such as L1 or L2 regularization which penalizes large weights, dropout so it would randomly \"dropping\" units during training.\n",
    "We can also consider reducing the number of layers or the number of hidden units so this can help the model generalize better by limiting its capacity to overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bigger Model Development\n",
    "\n",
    "To see the effect of model capacity, we created a bigger model by increasing the number of layers and units so the model could learn training data faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = keras.Sequential([ # Reference [4]\n",
    "    layers.Dense(64, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_big.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_big = model_big.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_big = keras.Sequential([ # Reference [4]\n",
    "    layers.Dense(64, activation='relu', input_shape=(10000,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_big.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_big = model_big.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting history of the bigger model\n",
    "history_dict = history_big.history\n",
    "training_accuracy = history_dict['accuracy']\n",
    "validation_accuracy = history_dict['val_accuracy']\n",
    "training_loss = history_dict['loss']\n",
    "validation_loss = history_dict['val_loss']\n",
    "\n",
    "epochs_range = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, training_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training & Validation Loss (Bigger Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, training_accuracy, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy (Bigger Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signs of Overfitting**\n",
    "\n",
    "With this bigger model we can see that the training loss drops amost to zero again by around epoch 10 and remains extremely low afterward. This indicates the model has learned to fit the training data very well. On the other hand, the validation loss initially goes down, around epochs 1-3 but starts to climb from about epoch 4 onward, eventually reaching values above 0.7. This divergence is a classic sign of overfitting because the model keeps fitting the training set more colesly but fails to generalize well to new, unseen data (validation set)\n",
    "\n",
    "The training accuracy shoots up quickly and plateaus around 98-100%, showing that the model again has almost \"memorized\" the training data. The validation accuracy improves initially (around epochs 1-3), then hovers around 85% range and even slightly decreases with more epochs. This widening gap between training accuracy (near-perfect) and validation accuracy (stalled in the mid-80s) further confirms overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization Experiments\n",
    "\n",
    "We will be adding Droput layers between the dense alyers to mitigate the overfitting that we are seeing on the previous experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "model_dropout = Sequential([\n",
    "    Input(shape=(10000,)),           \n",
    "    Dense(16, activation='relu'),   \n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_dropout = model_dropout.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting history\n",
    "history_dict = history_dropout.history\n",
    "training_accuracy = history_dict['accuracy']\n",
    "validation_accuracy = history_dict['val_accuracy']\n",
    "training_loss = history_dict['loss']\n",
    "validation_loss = history_dict['val_loss']\n",
    "\n",
    "epochs_range = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs_range, training_loss, 'bo-', label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, 'ro-', label='Validation Loss')\n",
    "plt.title('Training & Validation Loss (Bigger Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs_range, training_accuracy, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy (Bigger Model)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Investigation of Hyperparameter Settings [5]\n",
    "\n",
    "We can try to find the best hyperparameters manually with this Droput model. This approach systematically tests different combinations of:\n",
    "- Number of hidden units (16, 32, 64)\n",
    "- Number of Dense layers (e.g., 2 or 3 hidden layers)\n",
    "- Dropout rates (0.2, 0.5)\n",
    "- Optimizers (rmsprop, adam)\n",
    "- Batch sizes (128, 512, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by defining a function to build a model with variable hyperparameters\n",
    "def build_model(\n",
    "    num_layers=2,      # number of hidden layers\n",
    "    num_units=16,      # number of units per hidden layer\n",
    "    dropout_rate=0.5,  # dropout rate\n",
    "    optimizer='adam'   # optimizer: 'rmsprop' or 'adam'\n",
    "):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Input + first hidden layer\n",
    "    model.add(layers.Dense(num_units, activation='relu', input_shape=(num_words,)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional hidden layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(layers.Dense(num_units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we define the hyperparameter grids\n",
    "layers_list   = [2, 3]             \n",
    "units_list    = [16, 32, 64]       \n",
    "dropout_list  = [0.2, 0.5]         \n",
    "optimizers    = ['rmsprop', 'adam']\n",
    "batch_sizes   = [128, 512, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll store results here\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all combinations\n",
    "for n_layers in layers_list:\n",
    "    for units in units_list:\n",
    "        for dropout_rate in dropout_list:\n",
    "            for opt in optimizers:\n",
    "                for bsz in batch_sizes:\n",
    "                    \n",
    "                    # Build a fresh model\n",
    "                    model = build_model(\n",
    "                        num_layers=n_layers,\n",
    "                        num_units=units,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        optimizer=opt\n",
    "                    )\n",
    "                    \n",
    "                    # Train briefly (just to compare)\n",
    "                    history = model.fit(\n",
    "                        partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=7,           # short training for comparison\n",
    "                        batch_size=bsz,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=0          # turn off training logs\n",
    "                    )\n",
    "                    \n",
    "                    # Get final validation accuracy\n",
    "                    final_val_acc = history.history['val_accuracy'][-1]\n",
    "                    \n",
    "                    # Store the hyperparams + result\n",
    "                    results.append({\n",
    "                        'layers': n_layers,\n",
    "                        'units': units,\n",
    "                        'dropout': dropout_rate,\n",
    "                        'optimizer': opt,\n",
    "                        'batch_size': bsz,\n",
    "                        'val_acc': final_val_acc\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results by validation accuracy (descending) for easy viewing\n",
    "results_sorted = sorted(results, key=lambda x: x['val_acc'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 configurations\n",
    "print(\"Top 5 configurations:\")\n",
    "for r in results_sorted[:5]:\n",
    "    print(\n",
    "        f\"Layers={r['layers']}, \"\n",
    "        f\"Units={r['units']}, \"\n",
    "        f\"Dropout={r['dropout']}, \"\n",
    "        f\"Optimizer={r['optimizer']}, \"\n",
    "        f\"BatchSize={r['batch_size']} \"\n",
    "        f\"-> val_acc={r['val_acc']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recurrent Model CW2\n",
    "\n",
    "For this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Data Loading and Preprocessing\n",
    "# ----------------------------\n",
    "\n",
    "num_words = 10000\n",
    "maxlen = 500 #This will be the maximum lenght for the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and pad sequences\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. Build the Recurrent Model\n",
    "# ----------------------------\n",
    "model = keras.Sequential([\n",
    "    # Embedding layer converts integer indices into dense vectors\n",
    "    layers.Embedding(input_dim=num_words, output_dim=128),\n",
    "    \n",
    "    # Bidirectional LSTM layers capture sequence information from both directions\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    layers.Bidirectional(layers.LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    layers.Dropout(0.5),\n",
    "    \n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Create Training and Validation Splits\n",
    "# ----------------------------\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Advanced Best Practice: EarlyStopping Callback\n",
    "# ----------------------------\n",
    "# The EarlyStopping callback will monitor the validation loss and stop training\n",
    "# if it doesn't improve for 3 consecutive epochs, and it will restore the best model weights.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Train the Model with the Callback\n",
    "# ----------------------------\n",
    "history = model.fit(\n",
    "    partial_x_train,\n",
    "    partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. Evaluate the Model on Test Data\n",
    "# ----------------------------\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. Plot Training and Validation Metrics\n",
    "# ----------------------------\n",
    "history_dict = history.history\n",
    "epochs = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, history_dict['loss'], 'bo-', label='Training Loss')\n",
    "plt.plot(epochs, history_dict['val_loss'], 'ro-', label='Validation Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, history_dict['accuracy'], 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, history_dict['val_accuracy'], 'ro-', label='Validation Accuracy')\n",
    "plt.title('Training & Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
